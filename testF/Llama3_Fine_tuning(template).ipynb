{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOXtsV8z6aiH"
      },
      "outputs": [],
      "source": [
        "# 필요 Library install\n",
        "!pip install transformers==4.39.2 peft==0.10.0 trl==0.8.6 bitsandbytes==0.43.0 accelerate==0.29.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VKm5EZij6t2N"
      },
      "outputs": [],
      "source": [
        "# HF token 설정\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4rNgZM0MQos"
      },
      "outputs": [],
      "source": [
        "# 모델 경량화: Quantization 설정\n",
        "from transformers import BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "quantization_config=BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type='nf4'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNhIiAaBMXIU"
      },
      "outputs": [],
      "source": [
        "# 모델 경량화: Lora 설정\n",
        "from peft import LoraConfig\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,\n",
        "    r=16,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\",\"v_proj\",\"k_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRAxt0yjM9N1"
      },
      "outputs": [],
      "source": [
        "# 기본 Llama 3 모델 로드\n",
        "from transformers import AutoModelForCausalLM\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"meta-llama/Meta-Llama-3-8B\",\n",
        "    quantization_config = quantization_config,\n",
        "    device_map = {\"\": 0}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALbsxvDXAV41"
      },
      "outputs": [],
      "source": [
        "# Tokenizer 설정\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n",
        "tokenizer.add_special_tokens({\"pad_token\": \"<|reserved_special_token_250|>\"})\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqCHDCtJAicE"
      },
      "outputs": [],
      "source": [
        "# Prompt/Response Format 관련 설정\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "def convert_to_alpaca_format(instruction, response):\n",
        "    alpaca_format_str = f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\\n",
        "    \\n\\n### Instruction:\\n{instruction}\\n\\n### Response:\\n{response}\\\n",
        "    \"\"\"\n",
        "\n",
        "    return alpaca_format_str\n",
        "\n",
        "\n",
        "def prompt_formatting_func(examples):\n",
        "    instructions = examples[\"instruction\"]\n",
        "    outputs      = examples[\"output\"]\n",
        "    texts = []\n",
        "    for instruction, output in zip(instructions, outputs):\n",
        "        alpaca_formatted_str = convert_to_alpaca_format(instruction, output) + EOS_TOKEN\n",
        "        texts.append(alpaca_formatted_str)\n",
        "    return { \"text\" : texts, }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3LpzRjSEOLl"
      },
      "outputs": [],
      "source": [
        "# Dataset Load\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\")\n",
        "\n",
        "dataset = dataset.shuffle(seed=42)\n",
        "no_input_dataset = dataset.filter(lambda example: example['input'] == '')\n",
        "mapped_dataset = no_input_dataset.map(prompt_formatting_func, batched=True)\n",
        "split_dataset = mapped_dataset.train_test_split(test_size=0.01, seed=42)\n",
        "\n",
        "train_dataset = split_dataset['train']\n",
        "test_dataset = split_dataset['test']\n",
        "\n",
        "print(f\"Train dataset size: {len(train_dataset)}\")\n",
        "print(f\"Test dataset size: {len(test_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2st7_0JV7CdU"
      },
      "outputs": [],
      "source": [
        "# Data Collator 설정\n",
        "from trl import DataCollatorForCompletionOnlyLM\n",
        "data_collator_param = {}\n",
        "response_template = \"### Response:\\n\"\n",
        "collator = DataCollatorForCompletionOnlyLM(response_template=response_template, tokenizer=tokenizer, mlm=False)\n",
        "data_collator_param[\"data_collator\"] = collator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJNtEZ1gAVWW"
      },
      "outputs": [],
      "source": [
        "# local output dir 설정\n",
        "local_output_dir = \"/content/fine_tune_output\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9oR8G4NgAYh2"
      },
      "outputs": [],
      "source": [
        "!mkdir {local_output_dir}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6l-fadt2_FKk"
      },
      "outputs": [],
      "source": [
        "# tensorboard 설정\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir '{local_output_dir}/runs'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTChjxB3_H7G"
      },
      "outputs": [],
      "source": [
        "# Training setup\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "training_arguments = TrainingArguments(\n",
        "  output_dir=local_output_dir,\n",
        "  report_to = \"tensorboard\",\n",
        "  per_device_train_batch_size = 2,\n",
        "  per_device_eval_batch_size = 2,\n",
        "  gradient_accumulation_steps = 8,\n",
        "  warmup_steps = 50,\n",
        "  max_steps = 100,\n",
        "  eval_steps=10,\n",
        "  save_steps=50,\n",
        "  evaluation_strategy=\"steps\",\n",
        "  save_strategy=\"steps\",\n",
        "  learning_rate = 1e-4,\n",
        "  logging_steps = 1,\n",
        "  optim = \"adamw_8bit\",\n",
        "  weight_decay = 0.01,\n",
        "  lr_scheduler_type = \"constant_with_warmup\",\n",
        "  seed = 42,\n",
        "  gradient_checkpointing = True,\n",
        "  gradient_checkpointing_kwargs={'use_reentrant':True}\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = train_dataset,\n",
        "    eval_dataset = test_dataset,\n",
        "    peft_config=peft_config,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = 2048,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False,\n",
        "    args = training_arguments,\n",
        "    **data_collator_param\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKNP2hiV_KoD"
      },
      "outputs": [],
      "source": [
        "train_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJA3GiBU_MNr"
      },
      "outputs": [],
      "source": [
        "# Google drive로 복사\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7iCY-vKVAkCX"
      },
      "outputs": [],
      "source": [
        "!cp -r {local_output_dir} /content/drive/MyDrive"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
