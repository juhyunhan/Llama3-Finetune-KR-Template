{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOXtsV8z6aiH"
      },
      "outputs": [],
      "source": [
        "# 필요 Library install\n",
        "!pip install transformers==4.39.2 peft==0.10.0 trl==0.8.6 bitsandbytes==0.43.0 accelerate==0.29.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VKm5EZij6t2N"
      },
      "outputs": [],
      "source": [
        "# HF token 설정\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Google Drive Import\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "qkO0Kyw2tR-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tuned_model_path = \"/content/drive/MyDrive/fine_tune_output/checkpoint-50\""
      ],
      "metadata": {
        "id": "zoo1QIdStTem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 경량화: Quantization 설정\n",
        "from transformers import BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "quantization_config=BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type='nf4'\n",
        ")"
      ],
      "metadata": {
        "id": "d4rNgZM0MQos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 기본 Llama 3 모델 로드\n",
        "from transformers import AutoModelForCausalLM\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"meta-llama/Meta-Llama-3-8B\",\n",
        "    quantization_config = quantization_config,\n",
        "    device_map = {\"\": 0}\n",
        ")"
      ],
      "metadata": {
        "id": "lRAxt0yjM9N1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine Tune 된 모델 로드\n",
        "fine_tuned_model_cp_50 = AutoModelForCausalLM.from_pretrained(\n",
        "    fine_tuned_model_path,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map = {\"\": 0}\n",
        ")"
      ],
      "metadata": {
        "id": "DjDX3wmPr-8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALbsxvDXAV41"
      },
      "outputs": [],
      "source": [
        "# Tokenizer 로드\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(fine_tuned_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqCHDCtJAicE"
      },
      "outputs": [],
      "source": [
        "# Prompt/Response Format 관련 설정\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "def convert_to_alpaca_format(instruction, response):\n",
        "    alpaca_format_str = f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\\n",
        "    \\n\\n### Instruction:\\n{instruction}\\n\\n### Response:\\n{response}\\\n",
        "    \"\"\"\n",
        "\n",
        "    return alpaca_format_str"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(instruction_str, model):\n",
        "    inputs = tokenizer(\n",
        "    [\n",
        "        convert_to_alpaca_format(instruction_str,\"\",)\n",
        "    ], return_tensors = \"pt\").to(\"cuda\")\n",
        "    outputs = model.generate(**inputs, max_new_tokens = 128, use_cache = True, temperature = 0.05, top_p = 0.95)\n",
        "    return(tokenizer.batch_decode(outputs)[0])"
      ],
      "metadata": {
        "id": "dxOHbeMjpF4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3LpzRjSEOLl"
      },
      "outputs": [],
      "source": [
        "questions = [\n",
        "    \"List three ways to reduce plastic waste in daily life.\",\n",
        "    \"Write a haiku about artificial intelligence\",\n",
        "    \"How can I improve my public speaking skills?\",\n",
        "    \"AI 분야에서 사용하는 LLM이라는 용어가 뭔지 설명해줘\",\n",
        "    \"What is a famous tall tower in Paris?\",\n",
        "    \"What is Fine-Tuning?\",\n",
        "    \"15와 25의 최소공배수를 구하시오.\",\n",
        "    \"제2차 세계대전의 주요 원인은 무엇이었나요?\",\n",
        "    \"광합성 과정을 간단히 설명해주세요.\",\n",
        "    \"셰익스피어의 '햄릿'에서 주인공의 성격을 분석해보세요.\",\n",
        "    \"기후 변화가 전 세계적으로 미치는 영향에 대해 설명해주세요.\",\n",
        "    \"파이썬에서 리스트와 튜플의 차이점은 무엇인가요?\",\n",
        "    \"인공지능은 의식을 가질 수 있을까요? 그 이유는 무엇인가요?\",\n",
        "    \"'안녕하세요, 오늘 날씨가 좋네요'를 영어로 번역해주세요.\",\n",
        "    \"우주 여행을 주제로 한 짧은 이야기를 만들어보세요.\",\n",
        "    \"인플레이션이 경제에 미치는 영향을 설명해주세요.\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answers_dict = {\n",
        "  \"base_model_answers\": [],\n",
        "  \"fine_tuned_model_answers\": []\n",
        "}\n",
        "for idx, question in enumerate(questions):\n",
        "    print(f\"Processing EXAMPLE {idx}==============\")\n",
        "    base_model_output = test_model(question, base_model)\n",
        "    answers_dict['base_model_answers'].append(base_model_output)\n",
        "    fine_tuned_model_output = test_model(question, fine_tuned_model_cp_50)\n",
        "    answers_dict['fine_tuned_model_answers'].append(fine_tuned_model_output)"
      ],
      "metadata": {
        "id": "7iCY-vKVAkCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def simple_format(text, width=120):\n",
        "    return '\\n'.join(line[i:i+width] for line in text.split('\\n') for i in range(0, len(line), width))\n",
        "\n",
        "\n",
        "for idx, question in enumerate(questions):\n",
        "    print(f\"EXAMPLE {idx}==============\")\n",
        "    print(f\"Question: {question}\")\n",
        "\n",
        "    print(\"<<Base Model 답변>>\")\n",
        "    base_model_answer = answers_dict[\"base_model_answers\"][idx].split(\"### Response:\")[1]\n",
        "    print(simple_format(base_model_answer))\n",
        "    print(\"---\")\n",
        "    print(\"<<Fine Tuning Model 답변>>\")\n",
        "    fine_tuned_model_answer = answers_dict[\"fine_tuned_model_answers\"][idx].split(\"### Response:\")[1]\n",
        "    print(simple_format(fine_tuned_model_answer))\n",
        "    print()"
      ],
      "metadata": {
        "id": "jN9rOYCk2XhW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}